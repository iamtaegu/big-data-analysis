{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction - Korean\n",
    "## 오태건(20224071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corpus object\n",
    "corpus = [\n",
    "    \"반얀트리 서울, 스포츠 시설 리뉴얼 기념 농구 대회 개최\",\n",
    "    \"금융위에 P2P업체 투게더펀딩·펀다 온투업 등록 신청\",\n",
    "    \"문 닫힌 롯데백화점 본점\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # regular expression - substitution\n",
    "    #\\w : word character(a-z, A-Z, 0-9, ...)\n",
    "    #\\s : space\n",
    "    #[^...] : excluding...\n",
    "    doc = re.sub(r'[^\\w\\s]', ' ', doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [clean_doc(x) for x in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['반얀트리 서울  스포츠 시설 리뉴얼 기념 농구 대회 개최',\n",
       " '금융위에 P2P업체 투게더펀딩 펀다 온투업 등록 신청',\n",
       " '문 닫힌 롯데백화점 본점']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Tokenizers\n",
    "### (1) Okt - Open Korean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['반', '얀', '트리', '서울', '스포츠', '시설', '리뉴얼', '기념', '농구', '대회', '개최']\n",
      "['금융위', '에', 'P', '2', 'P', '업체', '투게더', '펀딩', '펀다', '온', '투업', '등록', '신청']\n",
      "['문', '닫힌', '롯데', '백화점', '본점']\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    tokens = okt.morphs(doc)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hannanum - KAIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "han = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['반얀트리', '서울', '스포츠', '시설', '리뉴얼', '기념', '농구', '대회', '개최']\n",
      "['금융위', '에', 'P2P업체', '투게더펀딩', '펀다', '온투업', '등록', '신청']\n",
      "['문', '닫히', 'ㄴ', '롯데백화점', '본점']\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    tokens = han.morphs(doc)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Kkma - SNU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['반', '얀', '트리', '서울', '스포츠', '시설', '리', '뉴', '얼', '기념', '농구', '대회', '개최']\n",
      "['금융', '위', '에', 'P', '2', 'P', '업체', '투', '것', '이', '더', '펀', '딩', '펀', '다', '오', 'ㄴ', '투', '업', '등록', '신청']\n",
      "['문', '닫히', 'ㄴ', '롯데', '백화점', '본점']\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    tokens = kkma.morphs(doc)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [\n",
    "    han.morphs(doc) for doc in corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = gensim.corpora.Dictionary(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '개최')\n",
      "(1, '기념')\n",
      "(2, '농구')\n",
      "(3, '대회')\n",
      "(4, '리뉴얼')\n",
      "(5, '반얀트리')\n",
      "(6, '서울')\n",
      "(7, '스포츠')\n",
      "(8, '시설')\n",
      "(9, 'P2P업체')\n",
      "(10, '금융위')\n",
      "(11, '등록')\n",
      "(12, '신청')\n",
      "(13, '에')\n",
      "(14, '온투업')\n",
      "(15, '투게더펀딩')\n",
      "(16, '펀다')\n",
      "(17, 'ㄴ')\n",
      "(18, '닫히')\n",
      "(19, '롯데백화점')\n",
      "(20, '문')\n",
      "(21, '본점')\n"
     ]
    }
   ],
   "source": [
    "for x in lexicon.items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Bag of Words(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n",
      "[(9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]\n",
      "[(17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n",
      "[(9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]\n",
      "[(17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    vec = [(x[0], 1) for x in vec]\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(dictionary = lexicon, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3333333333333333), (1, 0.3333333333333333), (2, 0.3333333333333333), (3, 0.3333333333333333), (4, 0.3333333333333333), (5, 0.3333333333333333), (6, 0.3333333333333333), (7, 0.3333333333333333), (8, 0.3333333333333333)]\n",
      "[(9, 0.35355339059327373), (10, 0.35355339059327373), (11, 0.35355339059327373), (12, 0.35355339059327373), (13, 0.35355339059327373), (14, 0.35355339059327373), (15, 0.35355339059327373), (16, 0.35355339059327373)]\n",
      "[(17, 0.4472135954999579), (18, 0.4472135954999579), (19, 0.4472135954999579), (20, 0.4472135954999579), (21, 0.4472135954999579)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    vec = tfidf[vec]\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
