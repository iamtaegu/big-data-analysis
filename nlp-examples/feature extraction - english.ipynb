{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오태건 (20224071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "### 1. nltk & gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    * 어근 뽑아내기 (Generator type function)\n",
    "        1. 소문자 변환\n",
    "        2. stemmer 인스턴스 생성\n",
    "        3. \n",
    "         3_1) 문장부호는 불필요하기 때문에 pass(continue)\n",
    "         3_2) stemmer.stem을 사용하여 어근 기준으로 파싱\n",
    "    * @return 어근으로 쪼개진(=token) 목록을 반환\n",
    "\"\"\"\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    \n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "\n",
    "        yield stemmer.stem(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elephant sneezed at the sight of potatoes.\n",
      "['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato']\n",
      "\n",
      "Bats can see via echolocation. See the bat sight sneeze!\n",
      "['bat', 'can', 'see', 'via', 'echoloc', 'see', 'the', 'bat', 'sight', 'sneez']\n",
      "\n",
      "Wondering, she opened the door to the studio.\n",
      "['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    print(doc)\n",
    "\n",
    "    tokens = list(tokenize(doc))\n",
    "    print(tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [\n",
    "    list(tokenize(doc)) for doc in corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato'],\n",
       " ['bat',\n",
       "  'can',\n",
       "  'see',\n",
       "  'via',\n",
       "  'echoloc',\n",
       "  'see',\n",
       "  'the',\n",
       "  'bat',\n",
       "  'sight',\n",
       "  'sneez'],\n",
       " ['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = gensim.corpora.Dictionary(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'at')\n",
      "(1, 'eleph')\n",
      "(2, 'of')\n",
      "(3, 'potato')\n",
      "(4, 'sight')\n",
      "(5, 'sneez')\n",
      "(6, 'the')\n",
      "(7, 'bat')\n",
      "(8, 'can')\n",
      "(9, 'echoloc')\n",
      "(10, 'see')\n",
      "(11, 'via')\n",
      "(12, 'door')\n",
      "(13, 'open')\n",
      "(14, 'she')\n",
      "(15, 'studio')\n",
      "(16, 'to')\n",
      "(17, 'wonder')\n"
     ]
    }
   ],
   "source": [
    "for x in lexicon.items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)]\n",
      "\n",
      "['bat', 'can', 'see', 'via', 'echoloc', 'see', 'the', 'bat', 'sight', 'sneez']\n",
      "[(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)]\n",
      "\n",
      "['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']\n",
      "[(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    print(doc)\n",
    "\n",
    "    # document to bag-of-words\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "\n",
    "    # sparse vector <=> dense vector\n",
    "    # sparse vector는 벡터중에 0은 표현하지 않고, 1의 위치만 작성하여 메모리 절약 \n",
    "    print(vec)\n",
    "    print()\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit & transform(predict)\n",
    "results = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "### 1. Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)]\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
      "\n",
      "[(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)]\n",
      "[(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
      "\n",
      "[(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "[(6, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    print(vec)\n",
    "\n",
    "    # 2번 나온 2값을 1로 변경\n",
    "    # (6,2) > (6,1)\n",
    "    vec = [(x[0], 1) for x in vec]\n",
    "    print(vec)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = Binarizer()\n",
    "vectors = onehot.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf\n",
    "\n",
    "### 1. Gensim\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    tf (t, d) &= 1 + \\log \\, f_{t,d}  \\\\\n",
    "    idf (t,D) &= \\log 1 + \\frac{N}{n_t} \\\\\n",
    "    tf-idf (t, d, D) &= tf (t,d) \\cdot idf (t,D)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(dictionary=lexicon, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x24126fedb90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato']\n",
      "[(0, 0.4837965208957426), (1, 0.4837965208957426), (2, 0.4837965208957426), (3, 0.4837965208957426), (4, 0.17855490118826325), (5, 0.17855490118826325)]\n",
      "\n",
      "['bat', 'can', 'see', 'via', 'echoloc', 'see', 'the', 'bat', 'sight', 'sneez']\n",
      "[(4, 0.10992597952954358), (5, 0.10992597952954358), (7, 0.5956913654963344), (8, 0.2978456827481672), (9, 0.2978456827481672), (10, 0.5956913654963344), (11, 0.2978456827481672)]\n",
      "\n",
      "['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']\n",
      "[(12, 0.408248290463863), (13, 0.408248290463863), (14, 0.408248290463863), (15, 0.408248290463863), (16, 0.408248290463863), (17, 0.408248290463863)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    print(doc)\n",
    "\n",
    "    vec = lexicon.doc2bow(doc) # bag of words\n",
    "    vec = tfidf[vec] # bow => tfidf\n",
    "\n",
    "    print(vec)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37867627, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.37867627, 0.37867627, 0.        , 0.37867627,\n",
       "        0.        , 0.        , 0.28799306, 0.        , 0.37867627,\n",
       "        0.        , 0.44730461, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.30251368, 0.30251368, 0.30251368, 0.        ,\n",
       "        0.30251368, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.60502736, 0.        , 0.23006945, 0.30251368, 0.        ,\n",
       "        0.        , 0.17866945, 0.        , 0.30251368, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.36772387,\n",
       "        0.        , 0.        , 0.        , 0.36772387, 0.        ,\n",
       "        0.        , 0.36772387, 0.        , 0.        , 0.        ,\n",
       "        0.36772387, 0.43436728, 0.36772387, 0.        , 0.36772387]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
